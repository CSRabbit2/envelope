= Configuration specification

This page specifies the configurations available in Envelope 0.4.

:toc:

== Example configuration

As illustration, a typical Envelope batch application that reads HDFS Avro files, extracts a subset of data, and writes
the results to S3 in Parquet might have the following configuration.

----
application {
  name = Envelope configuration example
  executors = 3
  executor.memory = 4G
}
steps {
  exampleInput {
    input {
      type = filesystem
      path = "hdfs://..."
      format = avro
    }
  }
  exampleStep {
    dependencies = [exampleInput]
    deriver {
      type = sql
      query.literal = "SELECT MY_UPPER(foo) AS foo FROM exampleInput WHERE MY_LOWER(bar) = 'blag'"
    }
    planner {
      type = append
    }
    output {
      type = filesystem
      path = "s3a://..."
      format = parquet
    }
  }
}
udfs : [
  {
    name = my_upper
    class = com...
  },
  {
    name = my_lower
    class = com...
  }
]
----

== Application

Application-level configurations have the `application.` prefix.

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|name
|The application name in YARN.

|executors
|The number of executors to be requested for the application. If not specified then Spark dynamic allocation will be used.

|executor.cores
|The number of cores per executor. Default is 1.

|executor.memory
|The amount of memory per executor. Default is 1G.

|batch.milliseconds
|The length of the micro-batch in milliseconds. Default is 1000. Ignored if the application does not have a streaming input.

|checkpoint.enabled
|Whether to use Spark checkpointing. Default is false. Ignored if the application does not have a streaming input.

|checkpoint.path
|The path for the Spark checkpoint. Ignored if checkpoint.enabled is not true.

|spark.conf.*
|Used to pass configurations directly to Spark. The `spark.conf.` prefix is removed and the configuration is set in the SparkConf object used to create the Spark context.

|===

== Steps

Step configurations have the `steps.[stepname].` prefix.

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|dependencies
|The list of step names that Envelope will submit before submitting this step.

|cache
|If `true` then Envelope will cache the step's DataFrame at the `MEMORY_ONLY` storage level. Default `false`.

|hint.small
|If `true` then Envelope will mark the step's DataFrame as small enough to be used in broadcast joins. Default `false`.

|===

=== Inputs

Input configurations have the `steps.[stepname].input.` prefix.

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|type
|The input type to be used. Envelope provides `filesystem`, `hive`, `jdbc`, `kafka`, `kudu`. To use a custom input, specify the fully qualified name of the `Input` implementation class.

|repartition.partitions
|The number of DataFrame partitions to repartition the input by. In Spark this will run `DataFrame#repartition`. If this configuration is not provided then Envelope will not repartition the input.

|repartition.columns
|(batch input only) A List of DataFrame columns to repartition the input by. In Spark this will run `DataFrame#repartition`. If this configuration is not provided then Envelope will not repartition the input. Per standard Spark convention, this function will repartition to the number of partitions defined by the Spark SQL configuration `spark.sql.shuffle.partitions` yet can be combined with the configuration `repartition.partitions` to change this default.  The list values must identify a DataFrame column name only; no expressions are evaluated.

|coalesce.partitions
|The number of DataFrame partitions to coalesce the input by. This configuration is only valid for batch inputs. In Spark this will run `DataFrame#coalesce`. If this configuration is not provided then Envelope will not coalesce the input.

||
|`_filesystem_`|

|path
|The Hadoop filesystem path to read as the input. Typically a Cloudera EDH will point to HDFS by default. Use `s3a://` for Amazon S3.

|format
|The file format of the files of the input directory. Envelope supports formats `parquet`, `json`, `csv`, and `input-format`.

|field.names
|(csv, json) List of StructType field names of the projected Row schema. In Spark, this will execute `DataFrameReader#schema`. For JSON, the field names must match the JSON data field names.

|field.types
|(csv, json) List of StructType field data types of the projected Row schema. In Spark, this will execute `DataFrameReader#schema`. For details, see the available options defined in <<Data Type Support>>.

|avro-schema.literal
|(csv, json) Inline Avro schema definition of the projected Row schema. In Spark, this will execute `DataFrameReader#schema`. For details, see the available options defined in <<Data Type Support>>.

|avro-schema.file
|(csv, json) A local (executor working directory) Avro schema file of the projected Row schema. In Spark, this will execute `DataFrameReader#schema`. For details, see the available options defined in <<Data Type Support>>.

|separator
|(csv) Spark option `sep`; sets the single character as a separator for each field and value. (default ,)

|encoding
|(csv) Spark option `encoding`; decodes the CSV files by the given encoding type. (default `UTF-8`)

|quote
|(csv) Spark option `quote`; sets the single character used for escaping quoted values where the separator can be part of the value. _If you would like to turn off quotations, you need to set not `null` but an empty string._ (default ")

|escape
|(csv) Spark option `escape`; sets the single character used for escaping quotes inside an already quoted value. (default \)

|comment
|(csv) Spark option `comment`; sets the single character used for skipping lines beginning with this character. By default, it is disabled. (default empty string)

|header
|(csv) Spark option `header`; uses the first line as names of columns. (default `false`)

|infer-schema
|(csv) Spark option `inferSchema`; infers the input schema automatically from data. It requires one extra pass over the data. (default `false`)

|ignore-leading-ws
|(csv) Spark option `ignoreLeadingWhiteSpace`; defines whether or not leading whitespaces from values being read should be skipped. (default `false`)

|ignore-trailing-ws
|(csv) Spark option `ignoreTrailingWhiteSpace`; defines whether or not trailing whitespaces from values being read should be skipped. (default `false`)

|null-value
|(csv) Spark option `nullValue`; sets the string representation of a null value. This applies to all supported types including the string type. (default empty string)

|nan-value
|(csv) Spark option `nanValue`; sets the string representation of a "non-number" value. (default `NaN`)

|positive-infinity
|(csv) Spark option `positiveInf`; sets the string representation of a positive infinity value. (default `Inf`)

|negative-infinity
|(csv) Spark option `negativeInf`; sets the string representation of a negative infinity value. (default `-Inf`)

|date-format
|(csv) Spark option `dateFormat`; sets the string that indicates a date format. Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to `date` type. (default `yyyy-MM-dd`)

|timestamp-format
|(csv) Spark option `timestampFormat`; sets the string that indicates a timestamp format. Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to `timestamp` type. (default `yyyy-MM-dd'T'HH:mm:ss.SSSZZ`)

|max-columns
|(csv) Spark option `maxColumns`; defines a hard limit of how many columns a record can have. (default `20480`)

|max-chars-per-column
|(csv) Spark option `maxCharsPerColumn`; defines the maximum number of characters allowed for any given value being read. By default, it is `-1` meaning unlimited length. (default `-1`)

|max-malformed-logged
|(csv) Spark option `maxMalformedLogPerPartition`; sets the maximum number of malformed rows Spark will log for each partition. Malformed records beyond this number will be ignored. (default `10`)

|mode
|(csv) Spark option `mode`; allows a mode for dealing with corrupt records during parsing.

`PERMISSIVE`: sets other fields to `null` when it meets a corrupted record. When a schema is set by user, it sets `null` for extra fields.

`DROPMALFORMED`: ignores the whole corrupted records.

`FAILFAST`: throws an exception when it meets corrupted records.

(default `PERMISSIVE`)

|format-class
|(input-format) The `org.apache.hadoop.mapreduce.InputFormat` canonical class name.

|key-class
|(input-format) The canonical class name for the InputFormat's keys.

|value-class
|(input-format) The canonical class name for the InputFormat's values.

|translator
|(input-format) The Translator class to use to convert the InputFormat's Key/Value pairs into Dataset Rows. See <<Translators>> for details.

||
|`_hive_`|

|table
|The Hive metastore table name (including database prefix, if required) to read as the input.

||
|`_jdbc_`|

|url
|The JDBC URL for the remote database.

|tablename
|The name of the table of the remote database to be read as the input.

|username
|The username to use to connect to the remote database.

|password
|The password to use to connect to the remote database.

||
|`_kafka_`|

|brokers
|The hosts and ports of the brokers of the Kafka cluster, in the form `host1:port1,host2:port2,...,hostn:portn`.

|topics
|The list of Kafka topics to be consumed.

|encoding
|The encoding of the messages in the Kafka topics, either `string` or `bytearray`. This must match the required encoding of the Envelope translator.

|window.enabled
|If `true` then Envelope will enable Spark Streaming windowing on the input. Ignored if the step does not contain a streaming input. Default `false`.

|window.milliseconds
|The duration in milliseconds of the Spark Streaming window for the input.

|parameter.*
|Used to pass configurations directly to Kafka. The `parameter.` prefix is removed and the configuration is set in the Kafka parameters map object used to create the Kafka direct stream.

||
|`_kudu_`|

|connection
|The hosts and ports of the masters of the Kudu cluster, in the form "host1:port1,host2:port2,...,hostn:portn".

|table.name
|The name of the Kudu table to be read as the input.

|===

=== Translators

Translator configurations have the `steps.[stepname].input.translator.` prefix.

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|type
|The translator type to be used. Envelope provides `avro`, `delimited`, `kvp`, `morphline`. To use a custom translator, specify the fully qualified name of the `Translator` implementation class.

||
|`_avro_`|

|field.names
|The list of fields to read from the Avro record.

|field.types
|The list of data types of the fields in the same order as the list of field names. Supported types are detailed in <<Data Type Support>>.

||
|`_delimited_`|

|delimiter
|The delimiter that separates the fields of the message.

|field.names
|The list of fields to read from the Avro record.

|field.types
|The list of data types of the fields in the same order as the list of field names. Supported types are detailed in <<Data Type Support>>.

||
|`_kvp_`|

|delimiter.kvp
|The delimiter that separates the key-value pairs of the message.

|delimiter.field
|The delimiter that separates the the key and value of each key-value pair.

|field.names
|The list of key names that will be found in the messages.

|field.types
|The list of data types of the fields in the same order as the list of field names. Supported types are detailed in <<Data Type Support>>.

||
|`_morphline_`|

|encoding.key
|The character set of the incoming key and is stored in the Record field, `_attachment_key_charset`. This must match the encoding of the Envelope input. The key value is stored in the field, `_attachment_key`.

|encoding.message
|The character set of the incoming message and is stored in the Record field, `_attachment_charset`. This must match the encoding of the Envelope input. The message value is stored in the field, `_attachment`.

|morphline.file
|The filename of the Morphline configuration found in the local directory of the executor. See the `--files` option for `spark-submit`.

|morphline.id
|The optional identifier of the Morphline pipeline within the configuration file.

|field.names
|The list of field names of the Record used to construct the output DataFrame, i.e. its StructType, and populate the Rows from the Record values.

|field.types
|The list of data types of the fields in the same order as the list of field names. Supported types are detailed in <<Data Type Support>>.

|===

=== Derivers

Deriver configurations have the `steps.[stepname].deriver.` prefix.

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|type
|The deriver type to be used. Envelope provides `morphline`, `nest`, `passthrough`, `sql`. To use a custom deriver, specify the fully qualified name of the `Deriver` implementation class.

|repartition.partitions
|The number of DataFrame partitions to repartition the deriver results by. In Spark this will run `DataFrame#repartition`. If this configuration is not provided then Envelope will not repartition the deriver results.

|coalesce.partitions
|The number of DataFrame partitions to coalesce the deriver results by. In Spark this will run `DataFrame#coalesce`. If this configuration is not provided then Envelope will not coalesce the deriver results.

||
|`_morphline_`|

|morphline.file
|The filename of the Morphline configuration found in the local directory of the executor. See the `--files` option for `spark-submit`.

|morphline.id
|The optional identifier of the Morphline pipeline within the configuration file.

|field.names
|The list of field names of the Record used to construct the output DataFrame, i.e. its StructType, and populate the Rows from the Record values.

|field.types
|The list of data types of the fields in the same order as the list of field names. Supported types are detailed in <<Data Type Support>>.

||
|`_nest_`|

|nest.into
|The name of the step whose records will be appended with the nesting of `nest.from`. Must be a dependency of the encapsulating step.

|nest.from
|The name of the step whose records will be nested into `nest.into`. Must be a dependency of the encapsulating step.

|key.field.names
|The list of field names that make up the common key of the two steps. This key will be used to determine which `nest.from` records will be nested into each `nest.into` record. There should only be one record in `nest.into` for each unique key of `nest.from`.

|nested.field.name
|The name to be given to the appended field that contains the nested records.

||
|`_passthrough_`
|_This deriver has no custom configurations_.

||
|`_sql_`|

|query.literal
|The literal query to be submitted to Spark SQL. Previously submitted steps can be referenced as tables by their step name.

|query.file
|The path to the file containing the query to be submitted to Spark SQL.

|===

=== Partitioners

Partitioner configurations have the `steps.[stepname].partitioner.` prefix.

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|type
|The partitioner type to be used. Envelope provides `hash`, `range`, `uuid`. To use a custom partitioner, specify the fully qualified name of the `ConfigurablePartitioner` implementation class. If no partitioner type is specified, Envelope will use the `hash` partitioner.

|===

=== Planners

Planner configurations have the `steps.[stepname].planner.` prefix.

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|type
|The planner type to be used. Envelope provides `append`, `bitemporal`, `eventtimeupsert`, `history`, `overwrite`, `upsert`. To use a custom planner, specify the fully qualified name of the `Planner` implementation class.

||
|`_append_`|

|fields.key
|The list of field names that make up the natural key of the record. Only required if `uuid.key.enabled` is true.

|field.last.updated
|The field name for the last updated attribute. If specified then Envelope will add this field and populate it with the system timestamp string.

|uuid.key.enabled
|If `true` then Envelope will overwrite the first key field with a UUID string.

||
|`_bitemporal_`|

|fields.key
|The list of field names that make up the natural key of the record.

|fields.values
|The list of field names that are used to determine if an arriving record is different to an existing record.

|field.timestamp
|The field name of the event time of the record. Must reference a field with the `LongType` Spark SQL data type.

|field.event.time.effective.from
|The field name of the event-time effective-from timestamp attribute on the output.

|field.event.time.effective.to
|The field name of the event-time effective-to timestamp attribute on the output.

|field.system.time.effective.from
|The field name of the system-time effective-from timestamp attribute on the output.

|field.system.time.effective.to
|The field name of the system-time effective-to timestamp attribute on the output.

|field.current.flag
|The field name of the current flag attribute on the output.

|carry.forward.when.null
|If `true` then Envelope will overwrite null values of the arriving record with the corresponding values of the most recent existing record for the same key.

||
|`_eventtimeupsert_`|

|fields.key
|The list of field names that make up the natural key of the record.

|field.last.updated
|The field name for the last updated attribute. If specified then Envelope will add this field and populate it with the system timestamp string.

|field.timestamp
|The field name of the event time of the record. Must reference a field with the `LongType` Spark SQL data type.

|field.values
|The list of field names that are used to determine if an arriving record is different to an existing record.

||
|`_history_`|

|fields.key
|The list of field names that make up the natural key of the record.

|fields.values
|The list of field names that are used to determine if an arriving record is different to an existing record.

|field.timestamp
|The field name of the event time of the record. Must reference a field with the `LongType` Spark SQL data type.

|field.effective.from
|The field name of the event-time effective-from timestamp attribute on the output.

|field.effective.to
|The field name of the event-time effective-to timestamp attribute on the output.

|field.current.flag
|The field name of the current flag attribute on the output.

|field.last.updated
|The field name for the last updated attribute. If specified then Envelope will add this field and populate it with the system timestamp string.

|carry.forward.when.null
|If `true` then Envelope will overwrite null values of the arriving record with the corresponding values of the most recent existing record for the same key.

||
|`_overwrite_`|_This deriver has no custom configurations_.

||
|`_upsert_`|

|field.last.updated
|The field name for the last updated attribute. If specified then Envelope will add this field and populate it with the system timestamp string.

|===

=== Outputs

Output configurations have the `steps.[stepname].output.` prefix.

[cols="2,8a", options="header"]
|===
|Configuration suffix|Description

|type
|The output type to be used. Envelope provides `filesystem`, `hive`, `jdbc`, `kafka`, `kudu`, `log` and `hbase`. To use a custom output, specify the fully qualified name of the `Output` implementation class.

||
|`_filesystem_`|

|path
|The Hadoop filesystem path to write as the output. Typically a Cloudera EDH will point to HDFS by default. Use `s3a://` for Amazon S3.

|format
|The file format for the files of the output directory. Envelope supports formats `parquet` and `csv`.

|partition.by
|The list of columns to partition the write output. Optional.

|separator
|(csv) Spark option `sep`; sets the single character as a separator for each field and value. (default ,)

|quote
|(csv) Spark option `quote`; sets the single character used for escaping quoted values where the separator can be part of the value. (default ")

|escape
|(csv) Spark option `escape`; sets the single character used for escaping quotes inside an already quoted value. (default \)

|escape-quotes
|(csv) Spark option `escapeQuotes`; a flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character. (default `true`)

|quote-all
|(csv) Spark option `quoteAll`; a flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character. (default `false`)

|header
|(csv) Spark option `header`; writes the names of columns as the first line. (default `false`)

|null-value
|(csv) Spark option `nullValue`; sets the string representation of a null value. (default empty string)

|compression
|(csv) Spark option `compression`; compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (`none`, `bzip2`, `gzip`, `lz4`, `snappy`, and `deflate`). (default `null`)

|date-format
|(csv) Spark option `dateFormat`; sets the string that indicates a date format. Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to `date` type. (default `yyyy-MM-dd`)

|timestamp-format
|(csv) Spark option `timestampFormat`; sets the string that indicates a timestamp format. Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to `timestamp` type. (default `yyyy-MM-dd'T'HH:mm:ss.SSSZZ`)

||
|`_hive_`|

|table
|The name of the Hive table to write to.

|partition.by
|The list of Hive table partition names to dynamically partition the write by. Optional.

||
|`_jdbc_`|

|url
|The JDBC URL for the remote database.

|tablename
|The name of the table of the remote database to write as the output.

|username
|The username to use to connect to the remote database.

|password
|The password to use to connect to the remote database.

||
|`_kafka_`|

|brokers
|The hosts and ports of the brokers of the Kafka cluster, in the form `host1:port1,host2:port2,...,hostn:portn`.

|topic
|The Kafka topic to write to.

|field.delimiter
|The delimiter string to separate the field values with. Default is `,`.

||
|`_kudu_`|

|connection
|The hosts and ports of the masters of the Kudu cluster, in the form "host1:port1,host2:port2,...,hostn:portn".

|table.name
|The name of the Kudu table to write to.

||
|`_log_`|

|delimiter
|The delimiter string to separate the field values with. Default is `,`.

|level
|The log4j level for the written logs. Default is `INFO`.

||
|`_hbase_`|

|table.name
|Required. The table for the output, specified in the format `[namespace:]name`, e.g. `envelopetest:test`.

|zookeeper
|Optional. In non-secure setups it is not a strict requirement to supply an hbase-site.xml file on the classpath,
so the ZooKeeper quorum can be specified with this property with the usual HBase configuration syntax. Note that
this will supersede any quorum specified in any hbase-site.xml file on the classpath.

|hbase.conf.*
|Optional. Pass-through options to set on the HBase connection. The `hbase.conf` prefix will be stripped. For example:

....
hbase {
  conf {
    hbase.client.retries.number = 5
    hbase.client.operation.timeout = 30000
  }
}
....

Note that non-String parameters are automatically cast to Strings, but the underlying HBase code will do any
required conversions from String.

|mapping.serde
|Optional. The fully qualified class name of the implementation to use when converting Spark `Row` objects into HBase `Put` s and `Get` s and
converting HBase `Result` s into `Row` s. Defaults to `default`, which is maps to `com.cloudera.labs.envelope.utils.hbase.HBase.DefaultMappingSerde`.
The default serde configuration syntax adheres as closely as possible to that of the
Spark-HBase DataSource at the expense of some additional functionality - this is with a view to
moving to the HBaseRelation at some point in the future.

|mapping.rowkey
|Required for `default` serde. The ordered list columns which comprise the HBase row key. These are expected to be separated by `rowkey.separator` in HBase, e.g. `["symbol", "transacttime"]`.

|mapping.rowkey.separator
|Optional. The separator to use when constructing the row key. This is interpreted as a Unicode string
so for binary separators use the `\uXXXX` syntax. Defaults to "`:`".

|mapping.columns
|Required for `default` serde. A map of column definitions specifying how to map Row fields into HBase columns. Each
column requires three attributes: the column family `cf`, the column qualifier `col` and
the column type `type`. The columns which comprise the row key are denoted with `cf = rowkey`.
Supported types are int, long, boolean, float, double and string. For example:

....
mapping.columns {
  symbol {
    cf = "rowkey"
    col = "symbol"
    type = "string"
  }
  transacttime {
    cf = "rowkey"
    col = "transacttime"
    type = "long"
  }
  clordid {
    cf = "cf1"
    col = "clordid"
    type = "string"
  }
  orderqty {
    cf = "cf1"
    col = "orderqty"
    type = "int"
  }
}
....

|batch.size
|Optional. An integer value with default 1000. The number of mutations to accumulate before making an HBase RPC call. For larger
cell sizes you may want to reduce this number or increase the relevant client buffers.

|===

== User-defined functions

Spark SQL user-defined functions (UDFs) are provided with a list of UDF specifications under `udfs`, where each specification has the following:

[cols="2,8", options="header"]
|===
|Configuration suffix|Description

|name
|The name of the UDF that will be used in SQL queries.

|class
|The fully qualified class name of the UDF implementation.

|===

== Data Type Support

Envelope supports the following Spark data types when defining a StructType schema inline (commonly via the `field.types` parameter):

* `string`
* `int`
* `long`
* `float`
* `double`
* `boolean`

When using an Avro schema to define the StructType, either via an inline Avro literal or a supporting Avro file, the following Spark data types are supported:

.Avro to StructType
|===
|Avro Type |Data Type

|record
|StructType

|array
|Array

|map
|Map (note: keys must be Strings)

|union
|StructType (each column representing the union elements, named `memberN`)

|bytes, fixed
|Binary

|string, enum
|String

|int
|Integer

|long
|Long

|float
|Float

|double
|Double

|boolean
|Boolean

|null
|Null

|date (LogicalType, as `long`)
|Date

|timestamp-millis (LogicalType, as `long`)
|Timestamp

|decimal (LogicalType, as `bytes`)
|Decimal
|===

