# Envelope User Guide

:toc:

## Fundamentals

Envelope runs as a graph of steps that are defined by the configuration. The key mapping back to Spark is that each Envelope step is a DataFrame, which is created from either an external input or a derivation of one or more other steps. Each step's DataFrame is registered by the step name as a Spark SQL temporary table so that subsequent queries can read directly from that step.

Each step can specify other steps as dependencies so that Envelope will wait for those steps to be submitted first. Steps that have no dependencies will run immediately. Steps that have common dependencies will run in parallel.

Each step can optionally write to an external output. The way that a step is applied to the output, e.g. to append, or to upsert, or to maintain a Type 2 SCD, is specified with a planner.

When at least one of the external input steps is a stream, e.g. Kafka, then the Envelope pipeline becomes a Spark Streaming job and the graph of steps will be executed every micro-batch.

## Configuration

Envelope can read Java properties, JSON, and https://github.com/typesafehub/config/blob/master/HOCON.md[HOCON] configuration files.

Each pipeline configuration file must define:
- One or more input steps with no dependencies
- One or more steps with an output
- One compatible planner for each output

And can additionally define:
- One application section for pipeline-level configurations
- One or more steps with one or more dependencies

For the full specification of Envelope configurations, see the link:docs/configurations.adoc[configuration guide] documentation page.